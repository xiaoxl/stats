{
  "hash": "ab3f0ae95b5afbbe5be052787dd7ac24",
  "result": {
    "engine": "knitr",
    "markdown": "# Inferences\n\n\n\n\n::: {.hidden}\n<!-- Constants and basic symbols -->\n\n$$\n\\require{physics}\n\\require{braket}\n$$\n\n$$\n\\newcommand{\\dl}[1]{{\\hspace{#1mu}\\mathrm d}}\n\\newcommand{\\me}{{\\mathrm e}}\n$$\n\n<!-- Probability -->\n\n$$\n \\newcommand{\\Exp}{\\operatorname{E}}\n \\newcommand{\\Var}{\\operatorname{Var}}\n \\newcommand{\\Mode}{\\operatorname{mode}}\n$$\n\n<!-- Distributions pdf -->\n\n$$\n \\newcommand{\\pdfbinom}{{\\tt binom}}\n \\newcommand{\\pdfbeta}{{\\tt beta}}\n \\newcommand{\\pdfpois}{{\\tt poisson}}\n \\newcommand{\\pdfgamma}{{\\tt gamma}}\n \\newcommand{\\pdfnormal}{{\\tt norm}}\n  \\newcommand{\\pdfexp}{{\\tt expon}}\n$$\n\n<!-- Distributions -->\n\n$$\n \\newcommand{\\distbinom}{\\operatorname{B}}\n \\newcommand{\\distbeta}{\\operatorname{Beta}}\n \\newcommand{\\distgamma}{\\operatorname{Gamma}}\n \\newcommand{\\distexp}{\\operatorname{Exp}}\n \\newcommand{\\distpois}{\\operatorname{Poisson}}\n \\newcommand{\\distnormal}{\\operatorname{\\mathcal N}}\n$$\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(getwd())\n#> [1] \"D:/Files/GoogleDrive/My writings/Onlinenotes/stats/contents/0\"\n```\n:::\n\n\n\n## General theory\n\nCited from [@Hog2019, pp. 206, Chapter 4].\n\n### Sampling\n\n\nConsider a random variable $X$ with an unknown distribution. Our information about the distribution of $X$ comes from a sample on $X$: $\\qty{X_1,\\ldots,X_n}$.\n\n- The sample ovservations $\\qty{X_1,\\ldots,X_n}$ have the same distribution as $X$.\n- $n$ denotes the **sample size**.\n- When the sample is actually drawn, we use $x_1,\\ldots,x_n$ as the **realizations** of the sample.\n\n\n::: {#def-}\n# Random sample\nIf the random variables $X_1,\\ldots, X_n$ are iid, then these random variable constitute a **random sample** of size $n$ from the common distribution.\n:::\n\n\n::: {#def-}\n# Statistics\nLet $X_1,\\ldots,X_n$ denote a sample on a random variable $X$. Let $T=T(X_1,\\ldots,X_n)$ be a function of the sample. $T$ is called a **statistic**. Once a sample is drawn, $t=T(x_1,\\ldots,x_n)$ is called the *realization* of $T$. \n:::\n\n\n::: {#def-}\n# Sampling distribution\n- The distribution of $T$ is called the **sampling distribution**.\n- The standard deviation of the sampling distribution is called the **standard error of estimate**.\n:::\n\n\n\n::: {#thm-}\n# The Central Limit Theorem\nFor large sample sizes, the mean $\\bar{y}$ of a sample from a population with mean $\\mu$ and a standard deviation $\\sigma$ has a sampling distribution that is approximately normal.\n:::\n\n### Point estimation\n\nAssume that the distribution of $X$ is known down to an unknown parameter $\\theta$ where $\\theta$ can be a vector. Then the pdf of $X$ can be written as $f(x;\\theta)$. In this case we might find some statistic $T$ to estimate $\\theta$. This is called a **point estimator** of $\\theta$. A realization $t$ is called an **estimate** of $\\theta$.\n\n\n::: {#def-}\n# Unbiasedness\nLet $X_1,\\ldots,X_n$ is a sample on a random varaible $X$ with pdf $f(x;\\theta)$. Let $T$ be a statistic. We say that $T$ is an **unbiased** estimator of $\\theta$ if $E(T)=\\theta$.\n:::\n\n\n\nLet $X$ be a random variable, with mean $\\mu$ and variance $\\sigma^2$. Consider a sample $\\set{X_i}$ of size $n$. By definition all $X_i$'s are iid. Therefore $\\Exp\\qty(X_i)=\\mu$, and $\\Var\\qty(X_i)=\\sigma^2$ for any $i=1,\\ldots, N$.\n\nConsider the following statistics:\n\n- $\\bar{\\mu}=\\dfrac1N\\sum_{i=1}^NX_i$,\n- $\\bar{\\sigma}^2=\\dfrac{1}{N-1}\\sum_{i=1}^N(X_i-\\bar{\\mu})^2$.\n\n\n::: {#lem-}\n\n1. $\\Exp(\\bar{\\mu})=\\mu$.\n2. $\\Exp(\\bar{\\sigma}^2)=\\sigma^2$.\n:::\n\n\n::: {.callout-tip collapse=\"true\"}\n# Proof\n$$\n\\begin{aligned}\n\\Exp\\qty(\\bar{\\mu})&=\\Exp\\qty(\\frac1N\\sum_{i=1}^NX_i)=\\frac1N\\sum_{i=1}^N\\Exp\\qty(X_i)=\\frac1N\\sum_{i=1}^N\\mu=\\mu,\\\\\n\\Exp\\qty(\\bar{\\sigma}^2)&=\\frac{1}{N-1}\\Exp\\qty[\\sum_{i=1}^N(X_i-\\bar{\\mu})^2]=\\frac{1}{N-1}\\sum_{i=1}^N\\Exp\\mqty[\\qty(X_i-\\bar{\\mu})^2]\\\\\n&=\\frac{1}{N-1}\\sum_{i=1}^N\\qty(\\Var\\qty(X_i-\\bar{\\mu})+\\qty(\\Exp\\qty(X_i-\\bar{\\mu}))^2)\\\\\n&=\\frac{1}{N-1}\\sum_{i=1}^N\\qty(\\Var\\qty(\\frac{N-1}{N}X_i-\\frac1NX_1-\\ldots-\\frac1NX_N)+\\qty(\\Exp\\qty(X_i)-\\Exp\\qty(\\bar{\\mu}))^2)\\\\\n&=\\frac{1}{N-1}\\sum_{i=1}^N\\qty(\\frac{(N-1)^2}{N^2}\\Var\\qty(X_i)+\\frac1{N^2}\\Var\\qty(X_1)+\\ldots+\\frac1{N^2}\\Var\\qty(X_N))\\\\\n&=\\frac{1}{N-1}\\sum_{i=1}^N\\qty(\\frac{(N-1)^2}{N^2}\\sigma^2+\\frac1{N^2}\\sigma^2+\\ldots+\\frac1{N^2}\\sigma^2)\\\\\n&=\\frac{N}{N-1}\\frac{(N-1)^2+N-1}{N^2}\\sigma^2=\\sigma^2.\n\\end{aligned}\n$$\n\n:::\n\n\n::: {#def-}\nThe following are the unbiased estimators of $\\mu$ and $\\sigma^2$ of $X$.\n\n1. $\\bar{\\mu}=\\dfrac1N\\sum_{i=1}^NX_i$ is called the *sample mean* of the samples. \n2. $\\bar{\\sigma}^2=\\dfrac{1}{N-1}\\sum_{i=1}^N(X_i-\\bar{\\mu})^2$ is called the *sample variance* of the samples.\n\n\n:::\n\n\n::: {.callout-caution}\nPlease pay attention to the denominator of the sample variance. The $N-1$ is due to the degree of freedom: all $X_i$'s and $\\bar{\\mu}$ are not independent to each other.\n:::\n\n\n\n### Confidence intervals\n\n::: {#def-}\n# Confidence interval\nConsider a sample of $X$. Fix a number $0<\\alpha<1$. Let $L$ and $U$ be two statistics. We say the interval $(L,U)$ is a $(1-\\alpha)100\\%$ **confidence interval** for $\\theta$ if \n\n$$\n1-\\alpha=\\Pr[\\theta\\in(L,U)].\n$$\n:::\n\n\n::: {#thm-}\n# Large-Sample $100(1-\\alpha)\\%$ Confidence interval\n$$\nL,U=\\bar{\\mu}\\pm z_{\\alpha/2}\\qty(\\frac{\\bar{\\sigma}}{\\sqrt{n}}),\n$$\nwhere $z_{\\alpha/2}=1.96$ if $\\alpha=5\\%$.\n\n:::\n\n- For any $n$, if $X_i\\sim \\mathcal N(\\mu, \\sigma^2)$, $T_n=\\dfrac{\\bar{X}-\\mu}{S/\\sqrt{n}}$ has a Student's $t$-distribution of degree of freedom $n-1$. \n- When $n$ is big enough, for any distribution $X_i$, $Z_n=\\dfrac{\\bar{X}-\\mu}{S/\\sqrt{n}}$ is approximately $\\mathcal N(0,1)$. \n- Student's $t$-distribution of degree of freedom $n-1$ is approaching $\\mathcal N(0,1)$ when $n$ is increasing. When $n=30$ they are very close to each other. Therefore in many cases Statisticians require sample size $\\geq30$.\n- For large sample or small sample, the coefficients to compute confidence intervals are $z_{\\alpha/2}$ or $t_{\\alpha/2}$. These two numbers come from normal distribution or Student's $t$-distribution.\n\n\n\n## Hypothesis test\nElements of a Statistical Test of Hypothesis \n\n- Null Hypothesis $𝐻_0$ \n- Alternative Hypothesis $𝐻_𝑎$ \n- Test Statistic \n- Level of significance $\\alpha$ \n- Rejection Region \n- $𝑃$-Value \n- Conclusion",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}