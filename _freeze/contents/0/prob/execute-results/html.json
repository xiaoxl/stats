{
  "hash": "a46ad9fb86f862a44664bb089b1d3b34",
  "result": {
    "engine": "knitr",
    "markdown": "\n\n::: {.hidden}\n<!-- Constants and basic symbols -->\n\n$$\n\\require{physics}\n\\require{braket}\n$$\n\n$$\n\\newcommand{\\dl}[1]{{\\hspace{#1mu}\\mathrm d}}\n\\newcommand{\\me}{{\\mathrm e}}\n$$\n\n<!-- Probability -->\n\n$$\n \\newcommand{\\Exp}{\\operatorname{E}}\n \\newcommand{\\Var}{\\operatorname{Var}}\n \\newcommand{\\Mode}{\\operatorname{mode}}\n$$\n\n<!-- Distributions pdf -->\n\n$$\n \\newcommand{\\pdfbinom}{{\\tt binom}}\n \\newcommand{\\pdfbeta}{{\\tt beta}}\n \\newcommand{\\pdfpois}{{\\tt poisson}}\n \\newcommand{\\pdfgamma}{{\\tt gamma}}\n \\newcommand{\\pdfnormal}{{\\tt norm}}\n  \\newcommand{\\pdfexp}{{\\tt expon}}\n$$\n\n<!-- Distributions -->\n\n$$\n \\newcommand{\\distbinom}{\\operatorname{B}}\n \\newcommand{\\distbeta}{\\operatorname{Beta}}\n \\newcommand{\\distgamma}{\\operatorname{Gamma}}\n \\newcommand{\\distexp}{\\operatorname{Exp}}\n \\newcommand{\\distpois}{\\operatorname{Poisson}}\n \\newcommand{\\distnormal}{\\operatorname{\\mathcal N}}\n$$\n:::\n\n\n\n\n\n# Probability theory\n\n## Notations\n- $Y$: a random variable (captial letters)\n- $y$: a sample of $Y$\n- $\\Pr\\qty(Y\\in A\\mid\\theta)$: the probability of $Y$ being in $A$\n- $p(y\\mid\\theta)=\\Pr\\qty(Y=y\\mid\\theta)$: the discrete probability density function\n- $f(y\\mid\\theta)=\\displaystyle\\dv{y}\\Pr\\qty(Y\\leq y\\mid\\theta)$: the continuous probability density function\n- $\\Exp\\qty(Y)$: the expectation of $Y$\n- $\\Var\\qty(Y)$: the variance of $Y$\n\n\n\n## Random variables\n\n\n::: {#def-}\n# Expectation\n$$\n\\Exp\\mqty[u(X)] = \\int_{-\\infty}^{\\infty}u(x)f(x)\\dl3x.\n$$\n:::\n\n\n::: {#def-}\n1. $\\mu=\\Exp(X)$ is called the **mean value** of $X$.\n2. $\\sigma^2=\\Var(X)=\\Exp\\mqty[(X-\\mu)^2]$ is called the **variance** of $X$.\n3. $M_X(t)=\\Exp\\mqty[\\me^{tX}]$ is called the **moment generating function** of $X$.\n:::\n\n\n::: {#prp-}\n1. $\\Exp\\mqty[ag(X)+bh(X)]=a\\Exp\\mqty[g(X)]+b\\Exp\\mqty[h(X)]$.\n2. $\\Var\\mqty[X]=\\Exp\\mqty[(X-\\mu)^2]=\\Exp(X^2)-\\mu^2$.\n:::\n\n::: {.proof}\n\n$$\n\\begin{split}\n\\Exp\\mqty[ag(X)+bh(X)]&=\\int_{-\\infty}^{\\infty}\\mqty[ag(x)+bh(x)]f(x)\\dl3x\\\\\n                 &=a\\int_{-\\infty}^{\\infty}g(x)f(x)\\dl3x+b\\int_{-\\infty}^{\\infty}h(x)f(x)\\dl3x\\\\\n                 &=a\\Exp\\mqty[g(X)]+b\\Exp\\mqty[h(X)].\n\\end{split}\n$$\n\n$$\n\\begin{split}\n\\Exp\\mqty[(X-\\mu)^2]&=\\Exp\\mqty[\\qty(X^2-2\\mu X+\\mu^2)]=\\Exp(X^2)-2\\mu\\Exp(X)+\\Exp(\\mu^2)\\\\\n&=\\Exp(X^2)-2\\mu\\mu+\\mu^2=\\Exp(X^2)-\\mu^2.\n\\end{split}\n$$\n:::\n\n### R code\nR has built-in random variables with different distributions. The naming convention is a prefix `d-`, `p-`, `q-` and `r-` together with the name of distribution. \n\n- `d-`: density function of the given distribution;\n- `p-`: cumulative density function of the given distribution;\n- `q-`: quantile function of the given distribution (which is the inverse of `p-` function);\n- `r-`: random sampling from the given distribution.\n\n\n::: {.callout-tip collapse=\"true\"}\n# Examples: normal distribution\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- seq(-4, 4, length=100)\ny <- dnorm(x, mean=2, sd=0.5)\nplot(x, y, type=\"l\")\n```\n\n::: {.cell-output-display}\n![](prob_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- seq(-4, 4, length=100)\ny <- pnorm(x, mean=2, sd=0.5)\nplot(x, y, type=\"l\")\n```\n\n::: {.cell-output-display}\n![](prob_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nqnorm(0)\n#> [1] -Inf\nqnorm(0.5)\n#> [1] 0\nqnorm(1)\n#> [1] Inf\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrnorm(10)\n#>  [1] -0.60831201 -0.26796877 -1.98091217  0.59579756 -1.41191930  0.42077236\n#>  [7] -1.62807633  0.01989833 -1.00843412  0.18417475\n```\n:::\n\n\n\n:::\n\n## Random vectors\n\n\n\n::: {#def-randomvector}\n# Random vector\nGiven a random experiment with a sample space $\\mathcal C$, consider two random variables $X_1$ and $X_2$, which assign to each element $c$ of $\\mathcal C$ one and only one ordered pair of numbers $X_1(c)=x_1$, $X_2(c)=x_2$. Then we say that $(X_1, X_2)$ is a **random vector**. The **space** of $(X_1, X_2)$ is the set of orderd pairs $\\mathcal D=\\set{(x_1, x_2)\\mid x_1=X_1(c), x_2=X_2(c), c\\in\\mathcal C}$.\n:::\n\n\n::: {#def-cdf}\n# Joint Cumulative Distribution Function\n**The joint cumulative distribution function** of $(X_1, X_2)$ is defined as follows.\n\n<!-- $$\nF_{X_1,X_2}(x_1,x_2)=\\pr\\sqb{\\set{X_1\\leq x_1}\\cap\\set{X_2\\leq x_2}}.\n$$\nIn continuous case,  -->\n$$\nF_{X_1,X_2}(x_1,x_2)=\\int_{-\\infty}^{x_1}\\int_{-\\infty}^{x_2}f_{X_1,X_2}(w_1,w_2)\\dl3w_1\\dl3w_2.\n$$\n:::\n\n\n<!-- ::: {#def-pdf}\n# Joint probability mass function (pmf)\nIn discrete random vector case, the **pmf** is defined as  \n$$\np_{X_1,X_2}(x_1,x_2)=\\pr\\sqb{X_1=x_1,X_2=x_2}.\n$$\n::: -->\n\n::: {#def-cdf}\n# Joint probability density function (pdf)\nIn continuous random vector case, the **pdf** is defined as  \n$$\nf_{X_1, X_2}(x_1, x_2)=\\frac{\\partial^2F_{X_1, X_2}(x_1,x_2)}{\\partial x_1\\partial x_2}.\n$$\n:::\n\n\n\n\n::: {#def-marginal}\n## Marginal pdf\nAssume $(X_1, X_2)$ be a continuous random vector. The **marginal pdf** is \n$$\nf_{X_1}(x_1)=\\int_{-\\infty}^{\\infty}f(x_1, x_2)\\dl3x_2.\n$$\n:::\n\n\n\n\n::: {#def-}\n# Expectation\nAssume that $Y=g(X_1, X_2)$. Then\n$$\n\\Exp(Y)=\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty} g(x_1, x_2) f_{X_1, X_2}(x_1,x_2)\\dl3x_1\\dl3x_2.\n$$\n:::\n\n\n\n\n\n::: {#def-conditionaldistribution}\n# Conditional probability\nThe conditional pdf is defined as follows:\n$$\nf_{X_1\\mid X_2}(x_1\\mid x_2)=\\frac{f_{X_1,X_2}(x_1,x_2)}{f_{X_1}(x_1)}=\\frac{f_{X_1,X_2}(x_1,x_2)}{\\int_{-\\infty}^{\\infty} f_{X_1,X_2}(x_1,w)\\dl3w},\n$$\nand the corresponding conditional probability is defined as\n$$\n\\Pr(X_1\\in A\\mid X_2=x_2)=\\int_Af_{X_1\\mid X_2}(x_1\\mid x_2)\\dl3x_1.\n$$\nThus $f_{X_1\\mid X_2}(x_1\\mid x_2)$ is a pdf of a random function of $X_1$.\n:::\n\n\n::: {#thm-}\nLet $(X_1, X_2)$ be a random vector such that $\\Var(X_2)$ is finite. $\\Exp(X_2\\mid X_1=x_1)$ and $\\Var(X_2\\mid X_1=x_1)$ can be seen as random functions of $X_1$. Then\n\na. $\\Exp\\mqty[\\Exp(X_2\\mid X_1)]=\\Exp(X_2)$.\nb. $\\Var\\mqty[\\Var(X_2\\mid X_1)]\\leq\\Var(X_2)$.\n:::\n\n\n\n### Relations to single variable case\nUnder the assumption of two variables $X$ and $Y$, when only talking about one variable $X$ (resp. $Y$), we are actually talking about the random variable corresponding to the marginal pdf. The ignoring the other variable part is handled by the integration part.\n\n\n::: {#prp-}\n1. $\\Exp_X\\mqty[u(X)]=\\Exp\\mqty[u(X)]$.\n2. $\\Var_X(X)=\\Var(X)$.\n:::\n\n\n::: {.proof}\n$$\n\\begin{aligned}\n\\Exp\\mqty[u(X)]&=\\iint u(x)f(x,y)\\dl3x\\dl3y=\\int u(x) \\mqty[\\displaystyle\\int f(x,y)\\dl3y]\\dl3x=\\int u(x)f_X(x)\\dl3x=\\Exp_X\\mqty[u(X)],\\\\\n\\Var(X)&=\\Exp\\mqty[(X-\\mu)^2]=\\Exp(X^2)-\\mu^2=\\Exp_X(X^2)-\\mu^2=\\Var_X(X).\n\\end{aligned}\n$$\n\n:::\n\n\n## Maximal likelihood estimation\nConsider the Bayes' Theorem \n\n$$\n    p(\\vb w\\mid \\mathcal D)=\\frac{p(\\mathcal D\\mid \\vb w)p(\\vb w)}{p(\\mathcal D)}.\n$$\n\n$p(\\mathcal D\\mid \\vb w)$ is called the *likelihood function*, $p(\\vb w)$ is called the *prior probability* and $p(\\vb w\\mid \\mathcal D)$ is called the *posterior probability*. A widely used frequentist estimator is *maximum likelihood*, in which $\\vb w$ is set to the value that maximizes the likelihood function $p(\\mathcal D\\mid \\vb w)$. Sometimes the likelihood function is changed to be the *error function* $-\\ln p$ and to maximize the likelihood function is the same as to minimize the error function.\n\n\n\nConsider a data set of observations $\\vb x=(x_1,x_2,\\ldots,x_N)^T$. These data are i.i.d., with respect to the Gaussian distribution $\\mathcal N(\\mu,\\sigma^2)$. Then we have the *likelihood function* if it is treated as a function of $\\mu$ and $\\sigma^2$:\n\n$$\n    p(\\vb x\\mid \\mu,\\sigma^2)=\\prod_{n=1}^N\\mathcal N(x_n\\mid \\mu,\\sigma^2).\n$$\nWe want to find $\\mu$ and $\\sigma^2$ to maximize the likelihood function. To do so, we would like to consider the error function \n\n$$\n\\begin{split}\n    -\\ln p(\\vb x\\mid \\mu,\\sigma^2)&=-\\ln \\prod_{n=1}^N\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp{-\\frac{1}{2\\sigma^2}(x_n-\\mu)^2}=\\sum_{n=1}^N\\qty(\\frac12\\ln(2\\pi\\sigma^2)+\\frac{1}{2\\sigma^2}(x_n-\\mu)^2)\\\\\n    &=\\frac{1}{2\\sigma^2}\\sum_{n=1}^N(x_n-\\mu)^2+\\frac{N}{2}\\ln(\\sigma^2)+\\frac{N}{2}\\ln(2\\pi).\n\\end{split}\n$$\n\nTake the derivative of it. We have\n\n$$\n   \\begin{split}\n       \\pdv{ \\qty(-\\ln p(\\vb x\\mid \\mu,\\sigma^2))}{\\mu}&=\\frac{1}{2\\sigma^2}\\sum_{n=1}^N2(x_n-\\mu)(-1)=-\\frac{1}{\\sigma^2}\\qty(N\\mu-\\sum_{n=1}^Nx_n),\\\\\n             \\pdv{ \\qty(-\\ln p(\\vb x\\mid \\mu,\\sigma^2))}{\\sigma^2}&=\\frac12(-1)(\\sigma^2)^{-2}\\qty(\\sum_{n=1}^N(x_n-\\mu)^2)+\\frac{N}{2}\\frac{1}{\\sigma^2}\\\\\n             &=-\\frac N{2(\\sigma^2)^2}\\qty(\\frac1N\\sum_{n-1}^N(x_n-\\mu)^2-\\sigma^2).\n   \\end{split} \n$$\nTo minimize the error function we need to let them be $0$. Then we have\n\n$$\n    \\mu_{ML}=\\sum_{n=1}^Nx_n,\\quad \\sigma^2_{ML}=\\frac1N\\sum_{n=1}^N(x_n-\\mu_{ML})^2.\n$$\n<!-- These are called the *sample mean* and the *sample variance* of the data. Note that the sample variance is measured with respect to the sample mean. -->\n\n\nCompute $\\Exp\\mqty[\\mu_{ML}]$ and $\\Exp\\mqty[\\sigma^2_{ML}]$.\n\n$$\n    \\Exp\\mqty[\\mu_{ML}]=\\Exp\\mqty[\\frac1N\\sum_{n=1}^Nx_n]=\\frac1N\\sum_{n=1}^N\\Exp\\mqty[x_n]=\\frac1NN\\mu=\\mu.\n$$\nSince $\\Var\\mqty[kx]=\\Exp\\mqty[(kx)^2]-(\\Exp\\mqty[kx])^2=k^2\\Exp\\mqty[x^2]-k^2\\Exp\\mqty[x]^2=k^2\\Var\\mqty[x]$, we have\n\n$$\n\\begin{aligned}\n    \\Var\\mqty[\\mu_{ML}]&=\\Var\\mqty[\\frac1N\\sum_{n=1}^Nx_n]=\\frac1{N^2}\\Var\\mqty[\\sum_{n=1}^Nx_n]=\\frac{1}{N^2}\\sum_{n=1}^N\\Var\\mqty[x_n]\\\\\n    &=\\frac{1}{N^2}(N\\sigma^2)=\\frac{1}{N}\\sigma^2,\\\\\n    \\Var\\mqty[x_n-\\mu_{ML}]&=\\Var\\mqty[\\frac{N-1}{N}x_n-\\frac1Nx_1-\\ldots-\\frac1Nx_N]\\\\\n    &=\\frac{(N-1)^2}{N^2}\\sigma^2+\\frac{1}{N^2}\\sigma^2+\\ldots+\\frac1{N^2}\\sigma^2\\\\\n    &=\\frac{N-1}{N}\\sigma^2.\n\\end{aligned}\n$$\nThen \n\n$$\n\\begin{split}\n    \\Exp\\mqty[\\sigma^2_{ML}]&=\\Exp\\mqty[\\frac1N\\sum_{n=1}^N(x_n-\\mu_{ML})^2]=\\frac1N\\sum_{n=1}^N\\Exp\\mqty[(x_n-\\mu_{ML})^2]\\\\\n    &=\\frac1N\\sum_{n=1}^N\\qty(\\Var\\mqty[x_n-\\mu_{ML}]+(\\Exp\\mqty[x_n-\\mu_{ML}])^2)=\\frac{N-1}{N}\\sigma^2.\n\\end{split}\n$$\n\n\nTherefore $\\sigma^2_{ML}$ is biased, and the unbiased variance estimation is \n\n$$\n    \\tilde{\\sigma}^2=\\frac{1}{N-1}\\sum_{n=1}^N(x_n-\\mu_{ML})^2.\n$$\n\n\n\n## \n\n\n::: {#thm-bayesthm}\n$$\nf_{X\\mid Y=y}(x)=\\frac{f_{Y\\mid X=x}(y\\mid x)f_X(x)}{f_Y(y)}\n$$\n:::\n\n\n\n::: {.callout-note}\n$f_{X\\mid Y}(x\\mid y)$ is a pdf w.r.t $X$, not a pdf w.r.t $Y$.\n:::\n\n\n\n\n::: {.cell-output-stdout}\n```\n# Heading 1\n\n## Heading 2\n\n```\n:::\n\n",
    "supporting": [
      "prob_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}