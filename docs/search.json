[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stats",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n[1]\n\n\nReferences\n\n\n[1] Hoff, P. D.\n(2009). A first course in bayesian statistical methods.\nSpringer.\n\n\n[2] Gareth James, T.\nH., Daniela Witten. (2023). An introduction to\nstatistical learning: With applications in python. Springer\nCham.\n\n\n[3] Wickham, H.\n(2014). Tidy data.\nJournal of Statistical Software 59.\n\n\n[4] Mendenhall, W.\nand Sincich, T. (2020). Regression\nanalysis: A second course in statistics. Pearson Education,\nInc.\n\n\n[5] Hogg, R. V.,\nMcKean, J. W. and Craig, A. T. (2019). Introduction to\nmathematical statistics. Pearson.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "contents/0/prob.html#notations",
    "href": "contents/0/prob.html#notations",
    "title": "1¬† Probability theory",
    "section": "1.1 Notations",
    "text": "1.1 Notations\n\n\\(Y\\): a random variable (captial letters)\n\\(y\\): a sample of \\(Y\\)\n\\(\\Pr\\qty(Y\\in A\\mid\\theta)\\): the probability of \\(Y\\) being in \\(A\\)\n\\(p(y\\mid\\theta)=\\Pr\\qty(Y=y\\mid\\theta)\\): the discrete probability density function\n\\(f(y\\mid\\theta)=\\displaystyle\\dv{y}\\Pr\\qty(Y\\leq y\\mid\\theta)\\): the continuous probability density function\n\\(\\Exp\\qty(Y)\\): the expectation of \\(Y\\)\n\\(\\Var\\qty(Y)\\): the variance of \\(Y\\)",
    "crumbs": [
      "Random variables",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Probability theory</span>"
    ]
  },
  {
    "objectID": "contents/0/prob.html#random-variables",
    "href": "contents/0/prob.html#random-variables",
    "title": "1¬† Probability theory",
    "section": "1.2 Random variables",
    "text": "1.2 Random variables\n\nDefinition 1.1 (Expectation) \\[\n\\Exp\\mqty[u(X)] = \\int_{-\\infty}^{\\infty}u(x)f(x)\\dl3x.\n\\]\n\n\nDefinition 1.2 ¬†\n\n\\(\\mu=\\Exp(X)\\) is called the mean value of \\(X\\).\n\\(\\sigma^2=\\Var(X)=\\Exp\\mqty[(X-\\mu)^2]\\) is called the variance of \\(X\\).\n\\(M_X(t)=\\Exp\\mqty[\\me^{tX}]\\) is called the moment generating function of \\(X\\).\n\n\n\nProposition 1.1 ¬†\n\n\\(\\Exp\\mqty[ag(X)+bh(X)]=a\\Exp\\mqty[g(X)]+b\\Exp\\mqty[h(X)]\\).\n\\(\\Var\\mqty[X]=\\Exp\\mqty[(X-\\mu)^2]=\\Exp(X^2)-\\mu^2\\).\n\n\n\nProof. \\[\n\\begin{split}\n\\Exp\\mqty[ag(X)+bh(X)]&=\\int_{-\\infty}^{\\infty}\\mqty[ag(x)+bh(x)]f(x)\\dl3x\\\\\n                 &=a\\int_{-\\infty}^{\\infty}g(x)f(x)\\dl3x+b\\int_{-\\infty}^{\\infty}h(x)f(x)\\dl3x\\\\\n                 &=a\\Exp\\mqty[g(X)]+b\\Exp\\mqty[h(X)].\n\\end{split}\n\\]\n\\[\n\\begin{split}\n\\Exp\\mqty[(X-\\mu)^2]&=\\Exp\\mqty[\\qty(X^2-2\\mu X+\\mu^2)]=\\Exp(X^2)-2\\mu\\Exp(X)+\\Exp(\\mu^2)\\\\\n&=\\Exp(X^2)-2\\mu\\mu+\\mu^2=\\Exp(X^2)-\\mu^2.\n\\end{split}\n\\]\n\n\n1.2.1 R code\nR has built-in random variables with different distributions. The naming convention is a prefix d-, p-, q- and r- together with the name of distribution.\n\nd-: density function of the given distribution;\np-: cumulative density function of the given distribution;\nq-: quantile function of the given distribution (which is the inverse of p- function);\nr-: random sampling from the given distribution.\n\n\n\n\n\n\n\nExamples: normal distribution\n\n\n\n\n\n\nx &lt;- seq(-4, 4, length=100)\ny &lt;- dnorm(x, mean=2, sd=0.5)\nplot(x, y, type=\"l\")\n\n\n\n\n\n\n\n\n\nx &lt;- seq(-4, 4, length=100)\ny &lt;- pnorm(x, mean=2, sd=0.5)\nplot(x, y, type=\"l\")\n\n\n\n\n\n\n\n\n\nqnorm(0)\n#&gt; [1] -Inf\nqnorm(0.5)\n#&gt; [1] 0\nqnorm(1)\n#&gt; [1] Inf\n\n\nrnorm(10)\n#&gt;  [1] -0.60831201 -0.26796877 -1.98091217  0.59579756 -1.41191930  0.42077236\n#&gt;  [7] -1.62807633  0.01989833 -1.00843412  0.18417475",
    "crumbs": [
      "Random variables",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Probability theory</span>"
    ]
  },
  {
    "objectID": "contents/0/prob.html#random-vectors",
    "href": "contents/0/prob.html#random-vectors",
    "title": "1¬† Probability theory",
    "section": "1.3 Random vectors",
    "text": "1.3 Random vectors\n\nDefinition 1.3 (Random vector) Given a random experiment with a sample space \\(\\mathcal C\\), consider two random variables \\(X_1\\) and \\(X_2\\), which assign to each element \\(c\\) of \\(\\mathcal C\\) one and only one ordered pair of numbers \\(X_1(c)=x_1\\), \\(X_2(c)=x_2\\). Then we say that \\((X_1, X_2)\\) is a random vector. The space of \\((X_1, X_2)\\) is the set of orderd pairs \\(\\mathcal D=\\set{(x_1, x_2)\\mid x_1=X_1(c), x_2=X_2(c), c\\in\\mathcal C}\\).\n\n\nDefinition 1.4 (Joint Cumulative Distribution Function) The joint cumulative distribution function of \\((X_1, X_2)\\) is defined as follows.\n\n\\[\nF_{X_1,X_2}(x_1,x_2)=\\int_{-\\infty}^{x_1}\\int_{-\\infty}^{x_2}f_{X_1,X_2}(w_1,w_2)\\dl3w_1\\dl3w_2.\n\\]\n\n\n\nDefinition 1.5 (Joint probability density function (pdf)) In continuous random vector case, the pdf is defined as\n\\[\nf_{X_1, X_2}(x_1, x_2)=\\frac{\\partial^2F_{X_1, X_2}(x_1,x_2)}{\\partial x_1\\partial x_2}.\n\\]\n\n\nDefinition 1.6 (Marginal pdf) Assume \\((X_1, X_2)\\) be a continuous random vector. The marginal pdf is \\[\nf_{X_1}(x_1)=\\int_{-\\infty}^{\\infty}f(x_1, x_2)\\dl3x_2.\n\\]\n\n\nDefinition 1.7 (Expectation) Assume that \\(Y=g(X_1, X_2)\\). Then \\[\n\\Exp(Y)=\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty} g(x_1, x_2) f_{X_1, X_2}(x_1,x_2)\\dl3x_1\\dl3x_2.\n\\]\n\n\nDefinition 1.8 (Conditional probability) The conditional pdf is defined as follows: \\[\nf_{X_1\\mid X_2}(x_1\\mid x_2)=\\frac{f_{X_1,X_2}(x_1,x_2)}{f_{X_1}(x_1)}=\\frac{f_{X_1,X_2}(x_1,x_2)}{\\int_{-\\infty}^{\\infty} f_{X_1,X_2}(x_1,w)\\dl3w},\n\\] and the corresponding conditional probability is defined as \\[\n\\Pr(X_1\\in A\\mid X_2=x_2)=\\int_Af_{X_1\\mid X_2}(x_1\\mid x_2)\\dl3x_1.\n\\] Thus \\(f_{X_1\\mid X_2}(x_1\\mid x_2)\\) is a pdf of a random function of \\(X_1\\).\n\n\nTheorem 1.1 Let \\((X_1, X_2)\\) be a random vector such that \\(\\Var(X_2)\\) is finite. \\(\\Exp(X_2\\mid X_1=x_1)\\) and \\(\\Var(X_2\\mid X_1=x_1)\\) can be seen as random functions of \\(X_1\\). Then\n\n\\(\\Exp\\mqty[\\Exp(X_2\\mid X_1)]=\\Exp(X_2)\\).\n\\(\\Var\\mqty[\\Var(X_2\\mid X_1)]\\leq\\Var(X_2)\\).\n\n\n\n1.3.1 Relations to single variable case\nUnder the assumption of two variables \\(X\\) and \\(Y\\), when only talking about one variable \\(X\\) (resp. \\(Y\\)), we are actually talking about the random variable corresponding to the marginal pdf. The ignoring the other variable part is handled by the integration part.\n\nProposition 1.2 ¬†\n\n\\(\\Exp_X\\mqty[u(X)]=\\Exp\\mqty[u(X)]\\).\n\\(\\Var_X(X)=\\Var(X)\\).\n\n\n\nProof. \\[\n\\begin{aligned}\n\\Exp\\mqty[u(X)]&=\\iint u(x)f(x,y)\\dl3x\\dl3y=\\int u(x) \\mqty[\\displaystyle\\int f(x,y)\\dl3y]\\dl3x=\\int u(x)f_X(x)\\dl3x=\\Exp_X\\mqty[u(X)],\\\\\n\\Var(X)&=\\Exp\\mqty[(X-\\mu)^2]=\\Exp(X^2)-\\mu^2=\\Exp_X(X^2)-\\mu^2=\\Var_X(X).\n\\end{aligned}\n\\]",
    "crumbs": [
      "Random variables",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Probability theory</span>"
    ]
  },
  {
    "objectID": "contents/0/prob.html#maximal-likelihood-estimation",
    "href": "contents/0/prob.html#maximal-likelihood-estimation",
    "title": "1¬† Probability theory",
    "section": "1.4 Maximal likelihood estimation",
    "text": "1.4 Maximal likelihood estimation\nConsider the Bayes‚Äô Theorem\n\\[\n    p(\\vb w\\mid \\mathcal D)=\\frac{p(\\mathcal D\\mid \\vb w)p(\\vb w)}{p(\\mathcal D)}.\n\\]\n\\(p(\\mathcal D\\mid \\vb w)\\) is called the likelihood function, \\(p(\\vb w)\\) is called the prior probability and \\(p(\\vb w\\mid \\mathcal D)\\) is called the posterior probability. A widely used frequentist estimator is maximum likelihood, in which \\(\\vb w\\) is set to the value that maximizes the likelihood function \\(p(\\mathcal D\\mid \\vb w)\\). Sometimes the likelihood function is changed to be the error function \\(-\\ln p\\) and to maximize the likelihood function is the same as to minimize the error function.\nConsider a data set of observations \\(\\vb x=(x_1,x_2,\\ldots,x_N)^T\\). These data are i.i.d., with respect to the Gaussian distribution \\(\\mathcal N(\\mu,\\sigma^2)\\). Then we have the likelihood function if it is treated as a function of \\(\\mu\\) and \\(\\sigma^2\\):\n\\[\n    p(\\vb x\\mid \\mu,\\sigma^2)=\\prod_{n=1}^N\\mathcal N(x_n\\mid \\mu,\\sigma^2).\n\\] We want to find \\(\\mu\\) and \\(\\sigma^2\\) to maximize the likelihood function. To do so, we would like to consider the error function\n\\[\n\\begin{split}\n    -\\ln p(\\vb x\\mid \\mu,\\sigma^2)&=-\\ln \\prod_{n=1}^N\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp{-\\frac{1}{2\\sigma^2}(x_n-\\mu)^2}=\\sum_{n=1}^N\\qty(\\frac12\\ln(2\\pi\\sigma^2)+\\frac{1}{2\\sigma^2}(x_n-\\mu)^2)\\\\\n    &=\\frac{1}{2\\sigma^2}\\sum_{n=1}^N(x_n-\\mu)^2+\\frac{N}{2}\\ln(\\sigma^2)+\\frac{N}{2}\\ln(2\\pi).\n\\end{split}\n\\]\nTake the derivative of it. We have\n\\[\n   \\begin{split}\n       \\pdv{ \\qty(-\\ln p(\\vb x\\mid \\mu,\\sigma^2))}{\\mu}&=\\frac{1}{2\\sigma^2}\\sum_{n=1}^N2(x_n-\\mu)(-1)=-\\frac{1}{\\sigma^2}\\qty(N\\mu-\\sum_{n=1}^Nx_n),\\\\\n             \\pdv{ \\qty(-\\ln p(\\vb x\\mid \\mu,\\sigma^2))}{\\sigma^2}&=\\frac12(-1)(\\sigma^2)^{-2}\\qty(\\sum_{n=1}^N(x_n-\\mu)^2)+\\frac{N}{2}\\frac{1}{\\sigma^2}\\\\\n             &=-\\frac N{2(\\sigma^2)^2}\\qty(\\frac1N\\sum_{n-1}^N(x_n-\\mu)^2-\\sigma^2).\n   \\end{split}\n\\] To minimize the error function we need to let them be \\(0\\). Then we have\n\\[\n    \\mu_{ML}=\\sum_{n=1}^Nx_n,\\quad \\sigma^2_{ML}=\\frac1N\\sum_{n=1}^N(x_n-\\mu_{ML})^2.\n\\] \nCompute \\(\\Exp\\mqty[\\mu_{ML}]\\) and \\(\\Exp\\mqty[\\sigma^2_{ML}]\\).\n\\[\n    \\Exp\\mqty[\\mu_{ML}]=\\Exp\\mqty[\\frac1N\\sum_{n=1}^Nx_n]=\\frac1N\\sum_{n=1}^N\\Exp\\mqty[x_n]=\\frac1NN\\mu=\\mu.\n\\] Since \\(\\Var\\mqty[kx]=\\Exp\\mqty[(kx)^2]-(\\Exp\\mqty[kx])^2=k^2\\Exp\\mqty[x^2]-k^2\\Exp\\mqty[x]^2=k^2\\Var\\mqty[x]\\), we have\n\\[\n\\begin{aligned}\n    \\Var\\mqty[\\mu_{ML}]&=\\Var\\mqty[\\frac1N\\sum_{n=1}^Nx_n]=\\frac1{N^2}\\Var\\mqty[\\sum_{n=1}^Nx_n]=\\frac{1}{N^2}\\sum_{n=1}^N\\Var\\mqty[x_n]\\\\\n    &=\\frac{1}{N^2}(N\\sigma^2)=\\frac{1}{N}\\sigma^2,\\\\\n    \\Var\\mqty[x_n-\\mu_{ML}]&=\\Var\\mqty[\\frac{N-1}{N}x_n-\\frac1Nx_1-\\ldots-\\frac1Nx_N]\\\\\n    &=\\frac{(N-1)^2}{N^2}\\sigma^2+\\frac{1}{N^2}\\sigma^2+\\ldots+\\frac1{N^2}\\sigma^2\\\\\n    &=\\frac{N-1}{N}\\sigma^2.\n\\end{aligned}\n\\] Then\n\\[\n\\begin{split}\n    \\Exp\\mqty[\\sigma^2_{ML}]&=\\Exp\\mqty[\\frac1N\\sum_{n=1}^N(x_n-\\mu_{ML})^2]=\\frac1N\\sum_{n=1}^N\\Exp\\mqty[(x_n-\\mu_{ML})^2]\\\\\n    &=\\frac1N\\sum_{n=1}^N\\qty(\\Var\\mqty[x_n-\\mu_{ML}]+(\\Exp\\mqty[x_n-\\mu_{ML}])^2)=\\frac{N-1}{N}\\sigma^2.\n\\end{split}\n\\]\nTherefore \\(\\sigma^2_{ML}\\) is biased, and the unbiased variance estimation is\n\\[\n    \\tilde{\\sigma}^2=\\frac{1}{N-1}\\sum_{n=1}^N(x_n-\\mu_{ML})^2.\n\\]",
    "crumbs": [
      "Random variables",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Probability theory</span>"
    ]
  },
  {
    "objectID": "contents/0/prob.html#section",
    "href": "contents/0/prob.html#section",
    "title": "1¬† Probability theory",
    "section": "1.5 ",
    "text": "1.5 \n\nTheorem 1.2 \\[\nf_{X\\mid Y=y}(x)=\\frac{f_{Y\\mid X=x}(y\\mid x)f_X(x)}{f_Y(y)}\n\\]\n\n\n\n\n\n\n\nNote\n\n\n\n\\(f_{X\\mid Y}(x\\mid y)\\) is a pdf w.r.t \\(X\\), not a pdf w.r.t \\(Y\\).\n\n\n\n# Heading 1\n\n## Heading 2",
    "crumbs": [
      "Random variables",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Probability theory</span>"
    ]
  },
  {
    "objectID": "contents/0/basics.html#data-structure",
    "href": "contents/0/basics.html#data-structure",
    "title": "2¬† Basics",
    "section": "2.1 Data structure",
    "text": "2.1 Data structure\nA dataset is a collection of values. Values are organized in two ways. Every value belongs to a variable and an observation.\n\nDefinition 2.1 (Variables and Observations [1]) ¬†\n\nA variable contains all values that measure the same underlying attribute (like height, temperature, duration) across units.\nAn observation contains all values measured on the same unit (like a person, or a day, or a race) across attributes.\n\n\n\nDefinition 2.2 (Quantitative and Qualitative [2]) ¬†\n\nQuantitative data are observations measured on a naturally occurring numerical scale. It is also called numerical data.\nQualitative data are nonnumerical data that can only be classified into one of a group of categories. It is also called categorical data.\n\n\n\nDefinition 2.3 (Discrete and Continuous) ¬†\n\nDiscrete random variable: A random variable that assumes either a finite number of values or an infinite sequence of values such as 0, 1, 2‚Ä¶\nContinuous random variable: A random variable that may assume any numerical value in an interval or collection of intervals.\n\n\n\n\n\n\n\n\nRandom variables\n\n\n\nA variable in a dataset can be modeled by a random variable. The probability density function / probability mass function of the random variable can describe the distribution of all possible values of the variable in a dataset.\nThen to make a mearuement is the same as to take a sample from the random variable.",
    "crumbs": [
      "Random variables",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Basics</span>"
    ]
  },
  {
    "objectID": "contents/0/basics.html#data-visualization",
    "href": "contents/0/basics.html#data-visualization",
    "title": "2¬† Basics",
    "section": "2.2 Data Visualization",
    "text": "2.2 Data Visualization\n\n2.2.1 Qualitative (categorical) data\nUsually the most important is the class relative frequency: \\[\n\\text{class relative frequency}=\\frac{\\text{class frequency}}{n}.\n\\]\nTo display it, we could use table, bar chart or pie chart.\n\n\n\n\n\n\nExample: Possums\n\n\n\n\n\nAll the example about possums below come from the dataset here.\n\nRPython\n\n\n\ndf &lt;- read.csv('possum.csv')\nhead(df)\n#&gt;   site pop sex age headL skullW totalL tailL\n#&gt; 1    1 Vic   m   8  94.1   60.4   89.0  36.0\n#&gt; 2    1 Vic   f   6  92.5   57.6   91.5  36.5\n#&gt; 3    1 Vic   f   6  94.0   60.0   95.5  39.0\n#&gt; 4    1 Vic   f   6  93.2   57.1   92.0  38.0\n#&gt; 5    1 Vic   f   2  91.5   56.3   85.5  36.0\n#&gt; 6    1 Vic   f   1  93.1   54.8   90.5  35.5\n\n\n\n\nimport pandas as pd\ndf = pd.read_csv(\"possum.csv\")\ndf.head()\n#&gt;    site  pop sex  age  headL  skullW  totalL  tailL\n#&gt; 0     1  Vic   m  8.0   94.1    60.4    89.0   36.0\n#&gt; 1     1  Vic   f  6.0   92.5    57.6    91.5   36.5\n#&gt; 2     1  Vic   f  6.0   94.0    60.0    95.5   39.0\n#&gt; 3     1  Vic   f  6.0   93.2    57.1    92.0   38.0\n#&gt; 4     1  Vic   f  2.0   91.5    56.3    85.5   36.0\n\n\n\n\nWe first compute the frequency table of the variable pop:\n\ntable(df$pop)\n#&gt; \n#&gt; other   Vic \n#&gt;    58    46\n\nor relative frequency talbe:\n\ntable(df$pop)/length(df$pop)\n#&gt; \n#&gt;     other       Vic \n#&gt; 0.5576923 0.4423077\n\nThen we could draw the barplot of this variable:\n\nbarplot(table(df$pop))\n\n\n\n\n\n\n\n\nand the pie plot:\n\npie(table(df$pop))\n\n\n\n\n\n\n\n\nNote that table is handling the statistics, while barplot and pie draw on top of the result from table.\n\n\n\n\n\n2.2.2 Quantitative (numerical) data\nWe would like to use histogram to display these type of data. In other words, we split the range into small segments (called bins), and count the frequency or relative frequency of data falling into these bins.\n\n\n\n\n\n\nExample: Possums\n\n\n\n\n\n\ndf &lt;- read.csv('possum.csv')\nhead(df)\n#&gt;   site pop sex age headL skullW totalL tailL\n#&gt; 1    1 Vic   m   8  94.1   60.4   89.0  36.0\n#&gt; 2    1 Vic   f   6  92.5   57.6   91.5  36.5\n#&gt; 3    1 Vic   f   6  94.0   60.0   95.5  39.0\n#&gt; 4    1 Vic   f   6  93.2   57.1   92.0  38.0\n#&gt; 5    1 Vic   f   2  91.5   56.3   85.5  36.0\n#&gt; 6    1 Vic   f   1  93.1   54.8   90.5  35.5\n\nWe display the histogram of headL. We could use breaks to control the number of bins.\n\nhist(df$headL, breaks=10)\n\n\n\n\n\n\n\n\nThe function hist does not just draw the histogram. It also provide many infomation we might need.\n\nres &lt;- hist(df$headL, breaks=10)\n\n\n\n\n\n\n\nres\n#&gt; $breaks\n#&gt;  [1]  82  84  86  88  90  92  94  96  98 100 102 104\n#&gt; \n#&gt; $counts\n#&gt;  [1]  1  6  2 12 22 27 22  7  3  0  2\n#&gt; \n#&gt; $density\n#&gt;  [1] 0.004807692 0.028846154 0.009615385 0.057692308 0.105769231 0.129807692\n#&gt;  [7] 0.105769231 0.033653846 0.014423077 0.000000000 0.009615385\n#&gt; \n#&gt; $mids\n#&gt;  [1]  83  85  87  89  91  93  95  97  99 101 103\n#&gt; \n#&gt; $xname\n#&gt; [1] \"df$headL\"\n#&gt; \n#&gt; $equidist\n#&gt; [1] TRUE\n#&gt; \n#&gt; attr(,\"class\")\n#&gt; [1] \"histogram\"\n\n\n\n\n\n\n2.2.3 Some statistics\nFor a quantitative data, we would also like to compute some statistics: minx, max, quartiles, median and mean. In R, we could use summary to compute them, and use box plots to show them.\n\n\n\n\n\n\nExample: Possums\n\n\n\n\n\n\nsummary(df)\n#&gt;       site           pop                sex                 age       \n#&gt;  Min.   :1.000   Length:104         Length:104         Min.   :1.000  \n#&gt;  1st Qu.:1.000   Class :character   Class :character   1st Qu.:2.250  \n#&gt;  Median :3.000   Mode  :character   Mode  :character   Median :3.000  \n#&gt;  Mean   :3.625                                         Mean   :3.833  \n#&gt;  3rd Qu.:6.000                                         3rd Qu.:5.000  \n#&gt;  Max.   :7.000                                         Max.   :9.000  \n#&gt;                                                        NA's   :2      \n#&gt;      headL            skullW          totalL          tailL      \n#&gt;  Min.   : 82.50   Min.   :50.00   Min.   :75.00   Min.   :32.00  \n#&gt;  1st Qu.: 90.67   1st Qu.:54.98   1st Qu.:84.00   1st Qu.:35.88  \n#&gt;  Median : 92.80   Median :56.35   Median :88.00   Median :37.00  \n#&gt;  Mean   : 92.60   Mean   :56.88   Mean   :87.09   Mean   :37.01  \n#&gt;  3rd Qu.: 94.72   3rd Qu.:58.10   3rd Qu.:90.00   3rd Qu.:38.00  \n#&gt;  Max.   :103.10   Max.   :68.60   Max.   :96.50   Max.   :43.00  \n#&gt; \n\nWe could use the box plot to show these infomation. It consists of a box, two lines and possibly some points:\n\nThe box in the box plot extends from the lower quartile to the upper quartile. The difference between the upper quartile and the lower quartile is called the inter-quartile range (IQR).\nThe lines, known as whiskers, extends to one and a half times the interquartile range, but then they are limited to reaching actual data points.\nThe points, considered as outliers, are those which are not covered by the box and the lines.\n\n\nboxplot(df$headL)\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.2.4 Relations among multiple variables\nWe could show the relation between two variables in a scatter plot.\n\n\n\n\n\n\nExample: Possums\n\n\n\n\n\nThe cases that both variables are numerical continuous:\n\nplot(df$headL, df$skullW)\n\n\n\n\n\n\n\n\nThe cases that one variable is categorical:\n\nplot(as.factor(df$pop), df$skullW)\n\n\n\n\n\n\n\n\nNote that in this case, the categorical data has to be a factor. And once it is cast into a factor, the plot is multiple box plots for each category.\nWe can see pairwise plots for each pair of variables. Note that before the plot, we have to cast pop and sex into factors.\n\ndf$pop &lt;- as.factor(df$pop)\ndf$sex &lt;- as.factor(df$sex)\npairs(df)\n\n\n\n\n\n\n\n\nPair plot is very important since it can easily help us to find the expected relations between variables. We will use it a lot in regression analysis.",
    "crumbs": [
      "Random variables",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Basics</span>"
    ]
  },
  {
    "objectID": "contents/0/basics.html#p",
    "href": "contents/0/basics.html#p",
    "title": "2¬† Basics",
    "section": "2.3 p",
    "text": "2.3 p\n\nDefinition 2.4 (Population) A population data set is a collection (or set) of data measured on all experimental units of interest to you. [2]\n\n\nDefinition 2.5 (Sample) A sample is a subset of data selected from a population. [2]\n\n\nDefinition 2.6 (random sample) A random sample of \\(n\\) experimental units is one selected from the population in such a way that every different sample of size \\(n\\) has an equal probability of selection. [2]\n\n\n2.3.1 Inferential statistics\n\nDefinition 2.7 (Statistical inference [2]) ¬†\n\nA statistical inference is an estimate, prediction, or some other generatlization about a population based on information contianed in a sample.\nA measure of reliability is a statement about the degree of uncertainty associated with a statistical inference.\n\n\n\n\n\n\n\n\nInferential statistics\n\n\n\n\nIdentify population\nIdentify variable(s)\nCollect sample data\nInference about population based on sample\nMeasure of reliability for inference\n\n\n\n\n\n\n\n[1] Wickham, H. (2014). Tidy data. Journal of Statistical Software 59.\n\n\n[2] Mendenhall, W. and Sincich, T. (2020). Regression analysis: A second course in statistics. Pearson Education, Inc.",
    "crumbs": [
      "Random variables",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Basics</span>"
    ]
  },
  {
    "objectID": "contents/0/inferences.html#general-theory",
    "href": "contents/0/inferences.html#general-theory",
    "title": "3¬† Inferences",
    "section": "3.1 General theory",
    "text": "3.1 General theory\nCited from [1, Chapter 4].\n\n3.1.1 Sampling\nConsider a random variable \\(X\\) with an unknown distribution. Our information about the distribution of \\(X\\) comes from a sample on \\(X\\): \\(\\qty{X_1,\\ldots,X_n}\\).\n\nThe sample ovservations \\(\\qty{X_1,\\ldots,X_n}\\) have the same distribution as \\(X\\).\n\\(n\\) denotes the sample size.\nWhen the sample is actually drawn, we use \\(x_1,\\ldots,x_n\\) as the realizations of the sample.\n\n\nDefinition 3.1 (Random sample) If the random variables \\(X_1,\\ldots, X_n\\) are iid, then these random variable constitute a random sample of size \\(n\\) from the common distribution.\n\n\nDefinition 3.2 (Statistics) Let \\(X_1,\\ldots,X_n\\) denote a sample on a random variable \\(X\\). Let \\(T=T(X_1,\\ldots,X_n)\\) be a function of the sample. \\(T\\) is called a statistic. Once a sample is drawn, \\(t=T(x_1,\\ldots,x_n)\\) is called the realization of \\(T\\).\n\n\nDefinition 3.3 (Sampling distribution) ¬†\n\nThe distribution of \\(T\\) is called the sampling distribution.\nThe standard deviation of the sampling distribution is called the standard error of estimate.\n\n\n\nTheorem 3.1 (The Central Limit Theorem) For large sample sizes, the mean \\(\\bar{y}\\) of a sample from a population with mean \\(\\mu\\) and a standard deviation \\(\\sigma\\) has a sampling distribution that is approximately normal.\n\n\n\n3.1.2 Point estimation\nAssume that the distribution of \\(X\\) is known down to an unknown parameter \\(\\theta\\) where \\(\\theta\\) can be a vector. Then the pdf of \\(X\\) can be written as \\(f(x;\\theta)\\). In this case we might find some statistic \\(T\\) to estimate \\(\\theta\\). This is called a point estimator of \\(\\theta\\). A realization \\(t\\) is called an estimate of \\(\\theta\\).\n\nDefinition 3.4 (Unbiasedness) Let \\(X_1,\\ldots,X_n\\) is a sample on a random varaible \\(X\\) with pdf \\(f(x;\\theta)\\). Let \\(T\\) be a statistic. We say that \\(T\\) is an unbiased estimator of \\(\\theta\\) if \\(E(T)=\\theta\\).\n\nLet \\(X\\) be a random variable, with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Consider a sample \\(\\set{X_i}\\) of size \\(n\\). By definition all \\(X_i\\)‚Äôs are iid. Therefore \\(\\Exp\\qty(X_i)=\\mu\\), and \\(\\Var\\qty(X_i)=\\sigma^2\\) for any \\(i=1,\\ldots, N\\).\nConsider the following statistics:\n\n\\(\\bar{\\mu}=\\dfrac1N\\sum_{i=1}^NX_i\\),\n\\(\\bar{\\sigma}^2=\\dfrac{1}{N-1}\\sum_{i=1}^N(X_i-\\bar{\\mu})^2\\).\n\n\nLemma 3.1 ¬†\n\n\\(\\Exp(\\bar{\\mu})=\\mu\\).\n\\(\\Exp(\\bar{\\sigma}^2)=\\sigma^2\\).\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\Exp\\qty(\\bar{\\mu})&=\\Exp\\qty(\\frac1N\\sum_{i=1}^NX_i)=\\frac1N\\sum_{i=1}^N\\Exp\\qty(X_i)=\\frac1N\\sum_{i=1}^N\\mu=\\mu,\\\\\n\\Exp\\qty(\\bar{\\sigma}^2)&=\\frac{1}{N-1}\\Exp\\qty[\\sum_{i=1}^N(X_i-\\bar{\\mu})^2]=\\frac{1}{N-1}\\sum_{i=1}^N\\Exp\\mqty[\\qty(X_i-\\bar{\\mu})^2]\\\\\n&=\\frac{1}{N-1}\\sum_{i=1}^N\\qty(\\Var\\qty(X_i-\\bar{\\mu})+\\qty(\\Exp\\qty(X_i-\\bar{\\mu}))^2)\\\\\n&=\\frac{1}{N-1}\\sum_{i=1}^N\\qty(\\Var\\qty(\\frac{N-1}{N}X_i-\\frac1NX_1-\\ldots-\\frac1NX_N)+\\qty(\\Exp\\qty(X_i)-\\Exp\\qty(\\bar{\\mu}))^2)\\\\\n&=\\frac{1}{N-1}\\sum_{i=1}^N\\qty(\\frac{(N-1)^2}{N^2}\\Var\\qty(X_i)+\\frac1{N^2}\\Var\\qty(X_1)+\\ldots+\\frac1{N^2}\\Var\\qty(X_N))\\\\\n&=\\frac{1}{N-1}\\sum_{i=1}^N\\qty(\\frac{(N-1)^2}{N^2}\\sigma^2+\\frac1{N^2}\\sigma^2+\\ldots+\\frac1{N^2}\\sigma^2)\\\\\n&=\\frac{N}{N-1}\\frac{(N-1)^2+N-1}{N^2}\\sigma^2=\\sigma^2.\n\\end{aligned}\n\\]\n\n\n\n\nDefinition 3.5 The following are the unbiased estimators of \\(\\mu\\) and \\(\\sigma^2\\) of \\(X\\).\n\n\\(\\bar{\\mu}=\\dfrac1N\\sum_{i=1}^NX_i\\) is called the sample mean of the samples.\n\\(\\bar{\\sigma}^2=\\dfrac{1}{N-1}\\sum_{i=1}^N(X_i-\\bar{\\mu})^2\\) is called the sample variance of the samples.\n\n\n\n\n\n\n\n\nCaution\n\n\n\nPlease pay attention to the denominator of the sample variance. The \\(N-1\\) is due to the degree of freedom: all \\(X_i\\)‚Äôs and \\(\\bar{\\mu}\\) are not independent to each other.\n\n\n\n\n3.1.3 Confidence intervals\n\nDefinition 3.6 (Confidence interval) Consider a sample of \\(X\\). Fix a number \\(0&lt;\\alpha&lt;1\\). Let \\(L\\) and \\(U\\) be two statistics. We say the interval \\((L,U)\\) is a \\((1-\\alpha)100\\%\\) confidence interval for \\(\\theta\\) if\n\\[\n1-\\alpha=\\Pr[\\theta\\in(L,U)].\n\\]\n\n\nTheorem 3.2 (Large-Sample \\(100(1-\\alpha)\\%\\) Confidence interval) \\[\nL,U=\\bar{\\mu}\\pm z_{\\alpha/2}\\qty(\\frac{\\bar{\\sigma}}{\\sqrt{n}}),\n\\] where \\(z_{\\alpha/2}=1.96\\) if \\(\\alpha=5\\%\\).\n\n\nFor any \\(n\\), if \\(X_i\\sim \\mathcal N(\\mu, \\sigma^2)\\), \\(T_n=\\dfrac{\\bar{X}-\\mu}{S/\\sqrt{n}}\\) has a Student‚Äôs \\(t\\)-distribution of degree of freedom \\(n-1\\).\nWhen \\(n\\) is big enough, for any distribution \\(X_i\\), \\(Z_n=\\dfrac{\\bar{X}-\\mu}{S/\\sqrt{n}}\\) is approximately \\(\\mathcal N(0,1)\\).\nStudent‚Äôs \\(t\\)-distribution of degree of freedom \\(n-1\\) is approaching \\(\\mathcal N(0,1)\\) when \\(n\\) is increasing. When \\(n=30\\) they are very close to each other. Therefore in many cases Statisticians require sample size \\(\\geq30\\).\nFor large sample or small sample, the coefficients to compute confidence intervals are \\(z_{\\alpha/2}\\) or \\(t_{\\alpha/2}\\). These two numbers come from normal distribution or Student‚Äôs \\(t\\)-distribution.",
    "crumbs": [
      "Random variables",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Inferences</span>"
    ]
  },
  {
    "objectID": "contents/0/inferences.html#hypothesis-test",
    "href": "contents/0/inferences.html#hypothesis-test",
    "title": "3¬† Inferences",
    "section": "3.2 Hypothesis test",
    "text": "3.2 Hypothesis test\nElements of a Statistical Test of Hypothesis\n\nNull Hypothesis \\(ùêª_0\\)\nAlternative Hypothesis \\(ùêª_ùëé\\)\nTest Statistic\nLevel of significance \\(\\alpha\\)\nRejection Region\n\\(ùëÉ\\)-Value\nConclusion\n\n\n\n\n\n[1] Hogg, R. V., McKean, J. W. and Craig, A. T. (2019). Introduction to mathematical statistics. Pearson.",
    "crumbs": [
      "Random variables",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Inferences</span>"
    ]
  },
  {
    "objectID": "contents/0/statsmodels.html#estimation",
    "href": "contents/0/statsmodels.html#estimation",
    "title": "4¬† stats models",
    "section": "4.1 Estimation",
    "text": "4.1 Estimation\na statistic whose sampling distribution has the mean around the proposed value and a small variance. If the means are equal: unbiased estimator. No: biased.",
    "crumbs": [
      "Random variables",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>stats models</span>"
    ]
  },
  {
    "objectID": "contents/0/statsmodels.html#build-a-model-that-fits-the-data",
    "href": "contents/0/statsmodels.html#build-a-model-that-fits-the-data",
    "title": "4¬† stats models",
    "section": "4.2 build a model that fits the data",
    "text": "4.2 build a model that fits the data\npredicted value: \\(\\hat{y}\\), \\(\\hat{\\beta}\\), ‚Ä¶\n\nmodel: \\(y=\\Exp(y)+\\varepsilon\\)\n\n[2]\n\nwant to estimate \\(f\\): \\(Y=f(X)+\\varepsilon\\)\nwant to use \\(\\hat{f}\\) to estimate \\(f\\): \\(\\hat{Y}=\\hat{f}(X)\\).\n\n\n\n\n\n[1] Mendenhall, W. and Sincich, T. (2020). Regression analysis: A second course in statistics. Pearson Education, Inc.\n\n\n[2] Gareth James, T. H., Daniela Witten. (2023). An introduction to statistical learning: With applications in python. Springer Cham.",
    "crumbs": [
      "Random variables",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>stats models</span>"
    ]
  },
  {
    "objectID": "contents/1/notations.html#notations",
    "href": "contents/1/notations.html#notations",
    "title": "Distributions",
    "section": "Notations",
    "text": "Notations\n\n\\(Y\\): a random variable\n\\(\\Pr(Y\\mid\\theta)\\): the probability of \\(Y\\)\n\\(p(y\\mid\\theta)=\\Pr(Y=y\\mid\\theta)\\): the discrete probability density function\n\\(f(y\\mid\\theta)=\\dfrac{\\dl1}{\\dl1y}\\Pr(Y\\leq y\\mid\\theta)\\): the continuous probability density function\n\\(\\Exp\\qty(Y)\\): the expectation of \\(Y\\)\n\\(\\Var\\qty(Y)\\): the variance of \\(Y\\)",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "contents/1/notations.html#distributions",
    "href": "contents/1/notations.html#distributions",
    "title": "Distributions",
    "section": "Distributions",
    "text": "Distributions\n\n\\(\\distbinom(n,\\theta)\\): Binomial distribution.\n\\(\\distbeta(\\alpha,\\beta)\\): Beta distribution.\n\\(\\distpois(\\lambda)\\): Poisson distribution.\n\\(\\distgamma(\\lambda)\\): Gamma distribution.\n\\(\\distexp(\\lambda)\\): Exponential distribution.\n\\(\\distnormal(\\mu,\\sigma^2)\\): Nomral distribution.",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "contents/1/notations.html#pdfs",
    "href": "contents/1/notations.html#pdfs",
    "title": "Distributions",
    "section": "pdfs",
    "text": "pdfs\n\n\\(\\displaystyle \\pdfbinom(y, n, \\theta)=\\dbinom{n}{y} \\theta^{y}(1-\\theta)^{n-y}\\).\n\\(\\displaystyle \\pdfbeta(\\theta, \\alpha, \\beta)=\\dfrac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}\\).\n\\(\\displaystyle \\pdfpois\\)\n\\(\\displaystyle \\pdfgamma\\)\n\\(\\displaystyle \\pdfexp\\)\n\\(\\displaystyle \\pdfnormal\\)\n\n\n\n\n\n[1] Hoff, P. D. (2009). A first course in bayesian statistical methods. Springer.",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "contents/1/dist/binom.html#expected-values",
    "href": "contents/1/dist/binom.html#expected-values",
    "title": "5¬† Binomial distribution",
    "section": "5.1 Expected values",
    "text": "5.1 Expected values\n\nTheorem 5.1 Let \\(Y\\sim\\distbinom(n,\\theta)\\). Then \\[\n\\Exp\\qty(Y)=n\\theta.\n\\]\n\n\n\n\n\n\n\nProof.\n\n\n\n\n\n\\[\n\\begin{split}\n    \\Exp\\qty(Y)&=\\sum_{y=0}^n\\dbinom{n}{y}y\\theta^y(1-\\theta)^{n-y}\\\\\n    &=\\sum_{y=0}^n\\frac{n!}{y!(n-y)!}y\\theta^y(1-\\theta)^{n-y}\\\\\n        &=\\sum_{y=1}^n\\frac{n(n-1)!}{y(y-1)!(n-y)!}y\\theta^{1+(y-1)}(1-\\theta)^{n-y}\\\\\n    &=\\sum_{y=1}^nn\\theta\\frac{(n-1)!}{(y-1)!\\qty((n-1)-(y-1))!}\\theta^{y-1}(1-\\theta)^{(n-1)-(y-1)}\\\\\n        &=\\sum_{y=0}^{n-1}n\\theta\\frac{(n-1)!}{y!(n-1-y)!}\\theta^{y}(1-\\theta)^{(n-1)-y}\\\\\n    &=n\\theta\\sum_{y=0}^{n-1}\\dbinom{n-1}{y}\\theta^y(1-\\theta)^{n-1-y}\\\\\n    &=n\\theta(\\theta+1-\\theta)^{n-1}\\\\\n    &=n\\theta.\n\\end{split}\n\\]",
    "crumbs": [
      "Distributions",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Binomial distribution</span>"
    ]
  },
  {
    "objectID": "contents/1/dist/binom.html#variance",
    "href": "contents/1/dist/binom.html#variance",
    "title": "5¬† Binomial distribution",
    "section": "5.2 Variance",
    "text": "5.2 Variance\ndd",
    "crumbs": [
      "Distributions",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Binomial distribution</span>"
    ]
  },
  {
    "objectID": "contents/1/dist/beta.html#expected-values",
    "href": "contents/1/dist/beta.html#expected-values",
    "title": "6¬† Beta distribution",
    "section": "6.1 Expected values",
    "text": "6.1 Expected values\n\nTheorem 6.1 Let \\(\\theta\\sim\\distbeta(\\alpha,\\beta)\\). Then \\(\\Exp\\qty[\\theta]=\\dfrac{\\alpha}{\\alpha+\\beta}\\).\n\n\nProof. \\[\n\\begin{split}\n    \\Exp\\qty[\\theta]&=\\int_0^1\\theta\\cdot\\pdfbeta(\\alpha,\\beta)\\dl3\\theta=\\int_0^1\\theta\\cdot c\\cdot\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}\\dl3\\theta\\\\\n    &=\\int_0^1c\\cdot\\theta^{\\alpha}(1-\\theta)^{\\beta-1}\\dl3\\theta=\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\frac{\\Gamma(\\alpha+1)\\Gamma(\\beta)}{\\Gamma(\\alpha++\\beta+1)}\\\\\n    &=\\frac{\\Gamma(\\alpha+1)\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\alpha+\\beta+1)}=\\frac{\\alpha}{\\alpha+\\beta}.\n\\end{split}\n\\]",
    "crumbs": [
      "Distributions",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Beta distribution</span>"
    ]
  },
  {
    "objectID": "contents/1/dist/beta.html#binomial-and-beta",
    "href": "contents/1/dist/beta.html#binomial-and-beta",
    "title": "6¬† Beta distribution",
    "section": "6.2 Binomial and Beta",
    "text": "6.2 Binomial and Beta\nIf the prior distribution is Beta, and the sampling data is binomial, then the posterior is also Beta.\n\nTheorem 6.2 Let the prior distribution for \\(\\theta\\) be \\[\n\\Pr(\\theta)=\\pdfbeta(\\alpha, \\beta)=c\\,\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}\n\\] and the sampling data for \\(y\\) be \\[\n\\Pr\\qty(y\\mid\\theta)=\\dbinom{n}{y}\\theta^y(1-\\theta)^{n-y},\n\\] then the posterior distribution is again beta: \\[\n\\Pr\\qty(\\theta\\mid y)=\\pdfbeta(\\alpha+y, \\beta+n-y).\n\\]\n\n\nProof. \\[\n\\begin{split}\n    \\Pr\\qty(\\theta\\mid y)&=\\frac{\\Pr\\qty(y\\mid\\theta)\\Pr\\qty(\\theta)}{\\int_0^1\\Pr\\qty(y\\mid\\theta)\\Pr\\qty(\\theta)\\dl3\\theta}=c\\,\\theta^y(1-\\theta)^{n-y}\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}\\\\\n    &=c\\,\\theta^{y+\\alpha-1}(1-\\theta)^{n-y+\\beta-1}\\\\\n    &=\\pdfbeta(\\alpha+y,\\beta+n-y).\n\\end{split}\n\\]",
    "crumbs": [
      "Distributions",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Beta distribution</span>"
    ]
  },
  {
    "objectID": "contents/1/dist/poisson.html#relations-between-poisson-distribution-and-binomial-distribution",
    "href": "contents/1/dist/poisson.html#relations-between-poisson-distribution-and-binomial-distribution",
    "title": "8¬† Poisson distribution",
    "section": "8.1 Relations between Poisson distribution and binomial distribution",
    "text": "8.1 Relations between Poisson distribution and binomial distribution\nConsider the number of events happen during a fixed length period. This is a Poisson process. Assume that the expectation of the distribution is \\(\\theta\\). This means that we could expect \\(\\theta\\) events happening during the time. The distribution of the process is \\(\\pdfpois(y, \\theta)\\).\nWe may understand the process in terms of binomial distribution. We could split the time period into \\(n\\) pieces. The probability of that the event happens in one piece is \\(p\\). Then this process can be described by a binomial distribution \\(\\pdfbinom(y, n, p)\\). Its expectation value is \\(np\\). Then we have \\(np=\\theta\\). Therefore \\(p=\\theta/n\\).\n\nTheorem 8.3 Poisson distribution is the limit of binomial distribution:\n\\[\n\\lim_{n\\rightarrow\\infty}\\pdfbinom(y,n,\\theta/n)=\\pdfpois(y,\\theta).\n\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{split}\n\\lim_{n\\rightarrow\\infty}\\pdfbinom(y,n,\\theta/n)&= \\lim_{n\\rightarrow\\infty}\\binom{n}{y}\\qty(\\frac{\\theta}{n})^y\\qty(1-\\frac{\\theta}{n})^{n-y}\\\\\n&=\\lim_{n\\rightarrow\\infty}\\frac{n!}{y!(n-k)!}\\frac{\\theta^y}{n^y}\\qty(1-\\frac{\\theta}{n})^n\\qty(1-\\frac{\\theta}{n})^{-y}\\\\\n&=\\frac{\\theta^y}{y!}\\lim_{n\\rightarrow\\infty}\\mqty[\\frac{n(n-1)(n-2)\\ldots(n-y+1)}{n^y}\\qty(1-\\frac{\\theta}{n})^{-y}]\\lim_{n\\rightarrow\\infty}\\qty(1-\\frac{\\theta}{n})^n\\\\\n&=\\frac{\\theta^y}{y!}\\me^{-\\theta}\\\\\n&=\\pdfpois(y,\\theta).\n\\end{split}\n\\]",
    "crumbs": [
      "Distributions",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Poisson distribution</span>"
    ]
  },
  {
    "objectID": "contents/1/dist/gamma.html",
    "href": "contents/1/dist/gamma.html",
    "title": "9¬† Gamma distribution",
    "section": "",
    "text": "\\[\n\\require{physics}\n\\require{braket}\n\\]\n\\[\n\\newcommand{\\dl}[1]{{\\hspace{#1mu}\\mathrm d}}\n\\newcommand{\\me}{{\\mathrm e}}\n\\]\n\n\\[\n\\newcommand{\\Exp}{\\operatorname{E}}\n\\newcommand{\\Var}{\\operatorname{Var}}\n\\newcommand{\\Mode}{\\operatorname{mode}}\n\\]\n\n\\[\n\\newcommand{\\pdfbinom}{{\\tt binom}}\n\\newcommand{\\pdfbeta}{{\\tt beta}}\n\\newcommand{\\pdfpois}{{\\tt poisson}}\n\\newcommand{\\pdfgamma}{{\\tt gamma}}\n\\newcommand{\\pdfnormal}{{\\tt norm}}\n  \\newcommand{\\pdfexp}{{\\tt expon}}\n\\]\n\n\\[\n\\newcommand{\\distbinom}{\\operatorname{B}}\n\\newcommand{\\distbeta}{\\operatorname{Beta}}\n\\newcommand{\\distgamma}{\\operatorname{Gamma}}\n\\newcommand{\\distexp}{\\operatorname{Exp}}\n\\newcommand{\\distpois}{\\operatorname{Poisson}}\n\\newcommand{\\distnormal}{\\operatorname{\\mathcal N}}\n\\]\n\n\nDefinition 9.1 Let \\(\\mathcal Y=\\set{0,1,2,\\ldots}\\). The uncertain quantity \\(Y\\in\\mathcal Y\\) has a (denoted by \\(Y\\sim\\distgamma(a,b)\\) or \\(Y\\sim\\Gamma(a,b)\\)) if \\[\n\\Pr\\qty(Y=\\theta\\mid a,b)=\\pdfgamma(\\theta,a, b)=\\dfrac{b^a}{\\Gamma(a)}\\theta^{a-1}\\me^{-b\\theta},\\quad \\text{for }\\theta,a,b&gt;0.\n\\]\nHere\n\n\\(a\\) is the shape.\n\\(b\\) is the rate, which also represents effective sample size.\n\\(\\lambda=\\frac1b\\) is the scale.\n\n\n\n\n\n\n\n\nPython code\n\n\n\n\nfrom scipy.stats import gamma\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nrv = gamma(a=20, scale=1/2)\n\nplt.plot(rv.pdf(np.arange(100)))\n\n\n\n\n\n\n\n\n\nrv.interval(.95)\n\n(6.108259792701973, 14.835426785792794)\n\n\n\n\n\nTheorem 9.1 If \\(Y\\sim \\distgamma(a, b)\\), then\n\n\\(\\Exp\\qty[Y\\mid a, b]=a/b\\),\n\\(\\Var\\qty[Y\\mid a, b]=a/b^2\\),\n\\(\\Mode\\qty[Y\\mid  a, b]=\\)\n\n\n\n\n\n\n\n\nProof\n\n\n\n\\[\n\\begin{split}\n\\Exp\\qty[Y\\mid a,b]&= \\int_{0}^{\\infty}\\theta\\frac{b^a}{\\Gamma(a)}\\theta^{a-1}\\me^{-b\\theta}\\dl3\\theta\\\\\n&=\\int_{0}^{\\infty}\\frac{b^a}{\\Gamma(a)}\\theta^{(a+1)-1}\\me^{-b\\theta}\\dl3\\theta\\\\\n&=\\int_{0}^{\\infty}\\frac{\\Gamma(a+1)}{b\\Gamma(a)}\\frac{b^{a+1}}{\\Gamma(a+1)}\\theta^{(a+1)-1}\\me^{-b\\theta}\\dl3\\theta\\\\\n&=\\frac{\\Gamma(a+1)}{b\\Gamma(a)}\\int_{0}^{\\infty}\\pdfgamma(\\theta,a+1,b)\\dl3\\theta\\\\\n&=\\frac{a}{b},\\\\\n\\Exp\\qty[Y^2\\mid a,b]&= \\int_{0}^{\\infty}\\theta^2\\frac{b^a}{\\Gamma(a)}\\theta^{a-1}\\me^{-b\\theta}\\dl3\\theta\\\\\n&=\\int_{0}^{\\infty}\\frac{b^a}{\\Gamma(a)}\\theta^{(a+2)-1}\\me^{-b\\theta}\\dl3\\theta\\\\\n&=\\int_{0}^{\\infty}\\frac{\\Gamma(a+2)}{b^2\\Gamma(a)}\\frac{b^{a+2}}{\\Gamma(a+2)}\\theta^{(a+2)-1}\\me^{-b\\theta}\\dl3\\theta\\\\\n&=\\frac{\\Gamma(a+2)}{b^2\\Gamma(a)}\\int_{0}^{\\infty}\\pdfgamma(\\theta,a+2,b)\\dl3\\theta\\\\\n&=\\frac{a(a+1)}{b^2},\\\\\n\\Var\\qty[Y\\mid a, b]&=\\Exp\\qty[Y^2\\mid a, b]-\\qty(\\Exp\\qty[Y\\mid a, b])^2\\\\\n&=\\frac{a(a+1)}{b^2}-\\frac{a^2}{b^2}=\\frac{a}{b^2}.\n\\end{split}\n\\]\n\n\n\n\n\n\n\n\nVague prior\n\n\n\nAssume \\(\\epsilon&gt;0\\), then \\(\\distgamma(\\epsilon, \\epsilon)\\) represents a distribution without knowledge. Its mean is \\(1\\) and variance is \\(1/\\epsilon\\) which is very large.\nWith Poisson likelihood, the posterior mean is \\(\\dfrac{\\epsilon+\\sum y_i}{\\epsilon+n}\\approx \\dfrac{\\sum y_i}{n}\\) .",
    "crumbs": [
      "Distributions",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Gamma distribution</span>"
    ]
  },
  {
    "objectID": "contents/1/dist/normal.html#the-empirical-theorem",
    "href": "contents/1/dist/normal.html#the-empirical-theorem",
    "title": "10¬† Normal distribution",
    "section": "10.1 The empirical theorem",
    "text": "10.1 The empirical theorem",
    "crumbs": [
      "Distributions",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Normal distribution</span>"
    ]
  },
  {
    "objectID": "contents/1/dist/normal.html#python-implementation",
    "href": "contents/1/dist/normal.html#python-implementation",
    "title": "10¬† Normal distribution",
    "section": "10.2 Python implementation",
    "text": "10.2 Python implementation\n\n\nClick to expand.\n\nWe use the norm object from scipy.stats package.\n\nscipy.stats.norm(loc=mean, scale=standard_deviation) is to intialize a normal distribution object.\npdf(): probablitiy distribution function, both inputs and outputs are numpy arrays.\ncdf(): cumulative distribution function, both inputs and outputs are numpy arrays.\ninterval(confidence, loc=0, scale=1): confidence interval.\nppf(): Percent point function (inverse of cdf), both inputs and outputs are numpy arrays.\nrvs(size=1): random samplings.\n\n\n\nCode\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.linspace(norm.ppf(0.001), norm.ppf(0.999), 100)\ny = norm.pdf(x)\nplt.plot(x, y)\n_ = plt.title(\"pdf for Normal distribution (0, 1)\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.linspace(norm.ppf(0.001), norm.ppf(0.999), 100)\ny = norm.cdf(x)\nplt.plot(x, y)\n_ = plt.title(\"cdf for Normal distribution (0, 1)\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.linspace(norm.ppf(0.001), norm.ppf(0.999), 100)\ny = norm.pdf(x)\nplt.plot(x, y)\n\nsamples = norm.rvs(size=1000)\nplt.hist(samples, bins=20, density=True)\n_ = plt.title(\"histogram of random samplings for Normal distribution (0, 1)\")",
    "crumbs": [
      "Distributions",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Normal distribution</span>"
    ]
  },
  {
    "objectID": "contents/1/dist/normal.html#r-implementation",
    "href": "contents/1/dist/normal.html#r-implementation",
    "title": "10¬† Normal distribution",
    "section": "10.3 R implementation",
    "text": "10.3 R implementation\n\n\nClick to expand.\n\nsafljsdf",
    "crumbs": [
      "Distributions",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Normal distribution</span>"
    ]
  },
  {
    "objectID": "contents/2/slr.html",
    "href": "contents/2/slr.html",
    "title": "11¬† slr",
    "section": "",
    "text": "ddd",
    "crumbs": [
      "SLR",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>slr</span>"
    ]
  },
  {
    "objectID": "contents/app/specialfunctions.html#gamma-functions",
    "href": "contents/app/specialfunctions.html#gamma-functions",
    "title": "Appendix A: Special functions",
    "section": "A.1 Gamma functions",
    "text": "A.1 Gamma functions\n\nDefinition A.1 (Gamma function) Let \\(z\\) be any complex number that \\(\\mathfrak R(z)&gt;0\\). Then \\[\n\\Gamma(z)=\\int_0^{\\infty}t^{z-1}\\me^{-t}\\dl3t.\n\\tag{A.1}\\]\n\n\nTheorem A.1 \\[\n\\Gamma(z+1)=z\\Gamma(z).\n\\tag{A.2}\\]\n\n\nProof. \\[\n\\begin{split}\n    \\Gamma(z+1)&=\\int_0^{\\infty}t^{z+1-1}\\me^{-t}\\dl3t=-\\int_0^{\\infty}t^{z+1-1}\\dl3\\me^{-t}\\\\\n    &=-t\\me^{-t}\\biggr\\rvert_0^{\\infty}+\\int_0^{\\infty}\\me^{-t}\\dl3t^{z}=\\int_0^{\\infty}zt^{z-1}\\me^{-t}\\dl3t=z\\Gamma(z).\n\\end{split}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Special functions</span>"
    ]
  },
  {
    "objectID": "contents/app/specialfunctions.html#beta-functions",
    "href": "contents/app/specialfunctions.html#beta-functions",
    "title": "Appendix A: Special functions",
    "section": "A.2 Beta functions",
    "text": "A.2 Beta functions\n\nDefinition A.2 (Beta function) Let \\(z_1\\), \\(z_2\\) be two complex numbers that \\(\\mathfrak R(z_1),\\mathfrak R(z_2)&gt;0\\). Then \\[\nB(z_1,z_2)=\\int_0^1t^{z_1-1}(1-t)^{z_2-1}\\dl3t.\n\\tag{A.3}\\]\n\n\nTheorem A.2 (Relations between Beta functions and Gamma functions) \\[\n\\Gamma(\\alpha)\\Gamma(\\beta)=\\Gamma(\\alpha+\\beta)B(\\alpha, \\beta).\n\\tag{A.4}\\]\n\n\nProof. Use the following trick to change a product of two integrals into a double integral. \\[\n\\begin{align}\n\\Gamma(\\alpha)\\Gamma(\\beta)&=\\int_0^{\\infty}u^{\\alpha-1}\\me^{-u}\\dl3u\\int_0^{\\infty}v^{\\beta-1}\\me^{-v}\\dl3v\\\\\n&=\\int_0^{\\infty}\\int_0^{\\infty}u^{\\alpha-1}v^{\\beta-1}\\me^{-u-v}\\dl3u\\dl3v.\n\\end{align}\n\\]\nSet \\(u=st\\), \\(v=s-st\\). Then \\(s=u+v\\), \\(t=\\dfrac{u}{u+v}\\), and \\(\\abs{\\dfrac{\\partial(u,v)}{\\partial(s,t)}}=\\abs{\\mqty[t&s\\\\1-t&-s]}=s\\). Then \\[\\begin{split}\n    \\Gamma(\\alpha)\\Gamma(\\beta)&=\\int_0^{\\infty}\\int_0^{\\infty}u^{\\alpha-1}v^{\\beta-1}\\me^{-u-v}\\dl3u\\dl3v\\\\\n    &=\\int_{v=0}^{v=\\infty}\\int_{u=0}^{u=\\infty}(st)^{\\alpha-1}(s(1-t))^{\\beta-1}\\me^{-s}s\\dl3s\\dl3t\\\\\n    &=\\int_{t=0}^{t=1}\\int_{s=0}^{s=\\infty}s^{\\alpha+\\beta-1}t^{\\alpha-1}(1-t)^{\\beta-1}\\me^{-s}\\dl3s\\dl3t\\\\\n    &=\\int_{0}^{\\infty}s^{\\alpha+\\beta-1}\\me^{-s}\\dl3s\\int_{0}^{1}t^{\\alpha-1}(1-t)^{\\beta-1}\\dl3t\\\\\n    &=\\Gamma(\\alpha+\\beta)B(\\alpha,\\beta).\n\\end{split}\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Special functions</span>"
    ]
  }
]